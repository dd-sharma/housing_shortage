{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "523ae5dd-b8f0-49f7-bc1c-d17f21eb5052",
   "metadata": {},
   "source": [
    "# Data Cleaning & Wrangling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db7e6b40-54db-4494-bd07-f780054ff321",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import zipfile\n",
    "import os\n",
    "import geopandas as gpd\n",
    "import chardet\n",
    "import fiona"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "97469592-ddd6-453a-a30f-cd350e007007",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global Variables\n",
    "RAW_DATA_DIR = \"../data/raw_data\"\n",
    "CLEAN_DATA_DIR = \"../data/clean_data\"\n",
    "HOUSING_SUPPLY_ZIP = os.path.join(RAW_DATA_DIR, \"housing_supply/Residential_Construction_Permits_by_County.gdb.zip\")\n",
    "HOUSING_SUPPLY_DIR = os.path.join(RAW_DATA_DIR, \"housing_supply/Residential_Construction_Permits_by_County\")\n",
    "HOUSING_SUPPLY_OUTPUT = os.path.join(CLEAN_DATA_DIR, \"housing_supply/residential_construction_permits_2000_2022.csv\")\n",
    "\n",
    "POPULATION_HISTORICAL_DIR = os.path.join(RAW_DATA_DIR, \"demand/population/historical\")\n",
    "POPULATION_HISTORICAL_OUTPUT = os.path.join(CLEAN_DATA_DIR, \"demand/population/historical/msa_historical.csv\")\n",
    "\n",
    "POPULATION_PROJECTED_FILE = os.path.join(RAW_DATA_DIR, \"demand/population/projected/estimates_all_msas_2020-2023.xlsx\")\n",
    "POPULATION_PROJECTED_OUTPUT = os.path.join(CLEAN_DATA_DIR, \"demand/population/projected/msa_projected.csv\")\n",
    "\n",
    "WAGES_DIR = os.path.join(RAW_DATA_DIR, \"demand/economic/wages\")\n",
    "WAGES_OUTPUT = os.path.join(CLEAN_DATA_DIR, \"demand/economic/wages/msa_wages.csv\")\n",
    "\n",
    "UNEMPLOYMENT_DIR = os.path.join(RAW_DATA_DIR, \"demand/economic/unemployment_rate\")\n",
    "UNEMPLOYMENT_OUTPUT = os.path.join(CLEAN_DATA_DIR, \"demand/economic/unemployment_rate/msa_unemployment.csv\")\n",
    "\n",
    "MIGRATION_DIR = os.path.join(RAW_DATA_DIR, \"demand/migration\")\n",
    "MIGRATION_OUTPUT_DIR = os.path.join(CLEAN_DATA_DIR, \"demand/migration\")\n",
    "\n",
    "COST_BURDEN_DIR = os.path.join(RAW_DATA_DIR, \"affordability_metrics/cost_burden\")\n",
    "COST_BURDEN_OUTPUT_DIR = os.path.join(CLEAN_DATA_DIR, \"affordability_metrics/cost_burden\")\n",
    "\n",
    "HOUSING_PRICES_FILE = os.path.join(RAW_DATA_DIR, \"affordability_metrics/housing_costs/metro_housing_prices.csv\")\n",
    "HOUSING_PRICES_OUTPUT = os.path.join(CLEAN_DATA_DIR, \"affordability_metrics/housing_costs/metro_housing_prices.csv\")\n",
    "\n",
    "RENTAL_PRICES_FILE = os.path.join(RAW_DATA_DIR, \"affordability_metrics/housing_costs/metro_rental_prices.csv\")\n",
    "RENTAL_PRICES_OUTPUT = os.path.join(CLEAN_DATA_DIR, \"affordability_metrics/housing_costs/metro_rental_prices.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d98eb50-1ba7-4188-bf12-bbd4c8ef1279",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unzip_file(zip_file_path, extraction_dir):\n",
    "    \"\"\"\n",
    "    Unzips the specified file into the given directory.\n",
    "\n",
    "    Args:\n",
    "        zip_file_path (str): Path to the zip file.\n",
    "        extraction_dir (str): Directory where the file should be extracted.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(extraction_dir):\n",
    "        with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "            zip_ref.extractall(extraction_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ff7b469d-34bd-42ad-b718-5407896cc65d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_geodatabase(gdb_path):\n",
    "    \"\"\"\n",
    "    Reads a GeoDatabase and returns the first layer as a GeoDataFrame.\n",
    "\n",
    "    Args:\n",
    "        gdb_path (str): Path to the GeoDatabase file.\n",
    "\n",
    "    Returns:\n",
    "        gpd.GeoDataFrame: GeoDataFrame containing data from the first layer.\n",
    "    \"\"\"\n",
    "    layers = fiona.listlayers(gdb_path)\n",
    "    return gpd.read_file(gdb_path, layer=layers[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d85b631c-d0cb-474e-89f6-7a2d79cdb394",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_housing_supply(zip_file_path, extraction_dir, output_path):\n",
    "    \"\"\"\n",
    "    Cleans housing supply data from a GeoDatabase.\n",
    "\n",
    "    Args:\n",
    "        zip_file_path (str): Path to the zip file containing GeoDatabase.\n",
    "        extraction_dir (str): Directory to extract the GeoDatabase.\n",
    "        output_path (str): Path to save the cleaned data.\n",
    "    \"\"\"\n",
    "    unzip_file(zip_file_path, extraction_dir)\n",
    "    gdb_path = [os.path.join(extraction_dir, file) for file in os.listdir(extraction_dir) if file.endswith('.gdb')][0]\n",
    "    gdf = read_geodatabase(gdb_path)\n",
    "    year_columns = [col for col in gdf.columns if any(str(year) in col for year in range(2000, 2023))]\n",
    "    relevant_columns = [\"GEOID\", \"NAME\", \"STATE_NAME\"] + year_columns\n",
    "    gdf_subset = gdf[relevant_columns]\n",
    "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "    gdf_subset.to_csv(output_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4944b8b3-30e3-4c45-bdf1-9c3d0cdc48f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_population_historical(directory, csv_files, output_path):\n",
    "    \"\"\"\n",
    "    Cleans and combines historical population data from multiple CSV files.\n",
    "\n",
    "    Args:\n",
    "        directory (str): Path to the directory containing raw CSV files.\n",
    "        csv_files (list): List of CSV file names in the directory.\n",
    "        output_path (str): Path to save the combined cleaned dataset.\n",
    "    \"\"\"\n",
    "    combined_data = []\n",
    "    for file in csv_files:\n",
    "        file_path = os.path.join(directory, file)\n",
    "        df = pd.read_csv(file_path)\n",
    "        df['DATE'] = pd.to_datetime(df['DATE'])\n",
    "        population_col = [col for col in df.columns if col != 'DATE'][0]\n",
    "        df.rename(columns={population_col: 'POPULATION'}, inplace=True)\n",
    "        msa_name = file.replace(\"msa_\", \"\").replace(\".csv\", \"\")\n",
    "        df['MSA'] = msa_name\n",
    "        combined_data.append(df)\n",
    "    combined_df = pd.concat(combined_data, ignore_index=True)\n",
    "    combined_df.sort_values(by=['MSA', 'DATE'], inplace=True)\n",
    "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "    combined_df.to_csv(output_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bac1dd3c-b3c0-46e5-9fb3-67d75306c438",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_combine_save(directory, csv_files, output_path):\n",
    "    \"\"\"\n",
    "    General-purpose function to clean and combine data from multiple CSV files.\n",
    "\n",
    "    Args:\n",
    "        directory (str): Path to the directory containing raw CSV files.\n",
    "        csv_files (list): List of CSV file names in the directory.\n",
    "        output_path (str): Path to save the combined cleaned dataset.\n",
    "    \"\"\"\n",
    "    combined_data = []\n",
    "    for file in csv_files:\n",
    "        file_path = os.path.join(directory, file)\n",
    "        df = pd.read_csv(file_path)\n",
    "        combined_data.append(df)\n",
    "    combined_df = pd.concat(combined_data, ignore_index=True)\n",
    "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "    combined_df.to_csv(output_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2c9aa85d-8da9-48ad-a5bf-64bb4a74bbb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_population_projected(file_path, output_path):\n",
    "    \"\"\"\n",
    "    Cleans projected population data from an Excel file.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): Path to the raw projected population file.\n",
    "        output_path (str): Path to save the cleaned data.\n",
    "    \"\"\"\n",
    "    df = pd.read_excel(file_path, skiprows=3)\n",
    "    df.columns = [\"Geographic Area\", \"Base Population (2020)\", \"Population 2020\", \"Population 2021\", \"Population 2022\", \"Population 2023\"]\n",
    "    df.dropna(subset=[\"Geographic Area\"], inplace=True)\n",
    "    irrelevant_rows = [\"United States\", \".In Metropolitan Statistical Area\"]\n",
    "    df = df[~df[\"Geographic Area\"].isin(irrelevant_rows)]\n",
    "    df['Geographic Area'] = df['Geographic Area'].str.lstrip('.')\n",
    "    df = df[~df['Geographic Area'].str.contains('Division', na=False)]\n",
    "    df['Geographic Area'] = df['Geographic Area'].str.replace('Metro Area', '', regex=False).str.strip()\n",
    "    df = df.iloc[:396]\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "    df.to_csv(output_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "293d2679-3cc3-4ad8-b770-92c544d23181",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_migration(input_directory, output_directory):\n",
    "    \"\"\"\n",
    "    Cleans migration data files in the input directory.\n",
    "\n",
    "    Args:\n",
    "        input_directory (str): Path to the directory containing raw migration data.\n",
    "        output_directory (str): Path to save the cleaned data.\n",
    "    \"\"\"\n",
    "    os.makedirs(output_directory, exist_ok=True)\n",
    "    for file_name in os.listdir(input_directory):\n",
    "        if file_name.endswith('.csv'):\n",
    "            file_path = os.path.join(input_directory, file_name)\n",
    "            df = pd.read_csv(file_path)\n",
    "            df.dropna(axis=1, how='all', inplace=True)\n",
    "            df.drop(columns=['ucgid', 'GEO_ID'], errors='ignore', inplace=True)\n",
    "            cols_to_drop = [col for col in df.columns if col.endswith('M')]\n",
    "            df.drop(columns=cols_to_drop, inplace=True)\n",
    "            if 'NAME' in df.columns:\n",
    "                df['NAME'] = df['NAME'].str.replace('Metro Area', '', regex=False).str.strip()\n",
    "            cleaned_file_path = os.path.join(output_directory, file_name)\n",
    "            df.to_csv(cleaned_file_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f3feb22d-4657-46b9-83d3-cdad20ba5a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_cost_burden(input_dir, output_dir):\n",
    "    \"\"\"\n",
    "    Cleans cost burden data files.\n",
    "\n",
    "    Args:\n",
    "        input_dir (str): Path to the directory containing raw cost burden data.\n",
    "        output_dir (str): Path to save cleaned data.\n",
    "    \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    for file_name in os.listdir(input_dir):\n",
    "        if file_name.endswith('.csv'):\n",
    "            file_path = os.path.join(input_dir, file_name)\n",
    "            df = pd.read_csv(file_path)\n",
    "            df.rename(columns={'geoname': 'city'}, inplace=True)\n",
    "            df.drop(columns=['sumlevel'], errors='ignore', inplace=True)\n",
    "            df.to_csv(os.path.join(output_dir, file_name), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "af102886-4355-4cbe-9fbb-3516b47b3344",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_housing_prices(file_path, output_path):\n",
    "    \"\"\"\n",
    "    Cleans housing price data.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): Path to the raw housing price data file.\n",
    "        output_path (str): Path to save cleaned data.\n",
    "    \"\"\"\n",
    "    # Detect encoding\n",
    "    with open(file_path, 'rb') as f:\n",
    "        raw_data = f.read(10000)\n",
    "        detected_encoding = chardet.detect(raw_data)['encoding']\n",
    "\n",
    "    # Read the file using the detected encoding\n",
    "    df = pd.read_csv(file_path, delimiter='\\t', encoding=detected_encoding)\n",
    "\n",
    "    # Rename and clean columns\n",
    "    df.rename(columns={'Region': 'city'}, inplace=True)\n",
    "    df['city'] = df['city'].str.replace('metro area', '', regex=False).str.strip()\n",
    "\n",
    "    # Save the cleaned data\n",
    "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "    df.to_csv(output_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9d0bf79f-2741-4555-9f10-0a911d1504c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_rental_prices(input_path, output_path):\n",
    "    \"\"\"\n",
    "    Cleans rental price data.\n",
    "\n",
    "    Args:\n",
    "        input_path (str): Path to the raw rental price file.\n",
    "        output_path (str): Path to save cleaned data.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(input_path)\n",
    "    df.drop(columns=['RegionID', 'SizeRank'], errors='ignore', inplace=True)\n",
    "    df.to_csv(output_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2710ef4c-c80b-4496-a0b7-0fb8440bdace",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_combine_save_wage(directory, csv_files, output_path):\n",
    "    \"\"\"\n",
    "    Cleans and combines wage data.\n",
    "\n",
    "    Args:\n",
    "        directory (str): Path to the raw wage data directory.\n",
    "        csv_files (list): List of CSV files in the directory.\n",
    "        output_path (str): Path to save the cleaned combined dataset.\n",
    "    \"\"\"\n",
    "    combined_data = []\n",
    "    for file in csv_files:\n",
    "        file_path = os.path.join(directory, file)\n",
    "        df = pd.read_csv(file_path)\n",
    "        combined_data.append(df)\n",
    "    combined_df = pd.concat(combined_data, ignore_index=True)\n",
    "    combined_df.to_csv(output_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ba9fb6aa-fd27-40ba-ae78-a7efce1dce0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_combine_save_unemployment(directory, csv_files, output_path):\n",
    "    \"\"\"\n",
    "    Cleans and combines unemployment data.\n",
    "\n",
    "    Args:\n",
    "        directory (str): Path to raw unemployment data directory.\n",
    "        csv_files (list): List of CSV files in the directory.\n",
    "        output_path (str): Path to save the cleaned data.\n",
    "    \"\"\"\n",
    "    combined_data = []\n",
    "    for file in csv_files:\n",
    "        file_path = os.path.join(directory, file)\n",
    "        df = pd.read_csv(file_path)\n",
    "        combined_data.append(df)\n",
    "    combined_df = pd.concat(combined_data, ignore_index=True)\n",
    "    combined_df.to_csv(output_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9184cddb-81d4-499e-b327-ed439e5a7330",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\deeps\\anaconda3\\Lib\\site-packages\\pyogrio\\raw.py:198: RuntimeWarning: organizePolygons() received a polygon with more than 100 parts.  The processing may be really slow.  You can skip the processing by setting METHOD=SKIP.\n",
      "  return ogr_read(\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Migration dataset\n",
    "    clean_migration(\n",
    "        input_directory='../data/raw_data/demand/migration',\n",
    "        output_directory='../data/clean_data/demand/migration'\n",
    "    )\n",
    "\n",
    "    # Cost burden dataset\n",
    "    clean_cost_burden(\n",
    "        input_dir='../data/raw_data/affordability_metrics/cost_burden',\n",
    "        output_dir='../data/clean_data/affordability_metrics/cost_burden'\n",
    "    )\n",
    "\n",
    "    # Housing prices dataset\n",
    "    clean_housing_prices(\n",
    "        file_path='../data/raw_data/affordability_metrics/housing_costs/metro_housing_prices.csv',\n",
    "        output_path='../data/clean_data/affordability_metrics/housing_costs/metro_housing_prices.csv'\n",
    "    )\n",
    "\n",
    "    # Rental prices dataset\n",
    "    clean_rental_prices(\n",
    "        input_path='../data/raw_data/affordability_metrics/housing_costs/metro_rental_prices.csv',\n",
    "        output_path='../data/clean_data/affordability_metrics/housing_costs/metro_rental_prices.csv'\n",
    "    )\n",
    "\n",
    "    # Population historical dataset\n",
    "    clean_population_historical(\n",
    "        directory='../data/raw_data/demand/population/historical',\n",
    "        csv_files=[f for f in os.listdir('../data/raw_data/demand/population/historical') if f.endswith('.csv')],\n",
    "        output_path='../data/clean_data/demand/population/historical/msa_historical.csv'\n",
    "    )\n",
    "\n",
    "    # Population projected dataset\n",
    "    clean_population_projected(\n",
    "        file_path='../data/raw_data/demand/population/projected/estimates_all_msas_2020-2023.xlsx',\n",
    "        output_path='../data/clean_data/demand/population/projected/msa_projected.csv'\n",
    "    )\n",
    "\n",
    "    # Housing supply dataset\n",
    "    clean_housing_supply(\n",
    "        zip_file_path='../data/raw_data/housing_supply/Residential_Construction_Permits_by_County.gdb.zip',\n",
    "        extraction_dir='../data/raw_data/housing_supply/Residential_Construction_Permits_by_County',\n",
    "        output_path='../data/clean_data/housing_supply/residential_construction_permits_2000_2022.csv'\n",
    "    )\n",
    "\n",
    "    # Wages dataset\n",
    "    clean_combine_save_wage(\n",
    "        directory='../data/raw_data/demand/economic/wages',\n",
    "        csv_files=[f for f in os.listdir('../data/raw_data/demand/economic/wages') if f.endswith('.csv')],\n",
    "        output_path='../data/clean_data/demand/economic/wages/msa_wages.csv'\n",
    "    )\n",
    "\n",
    "    # Unemployment dataset\n",
    "    clean_combine_save_unemployment(\n",
    "        directory='../data/raw_data/demand/economic/unemployment_rate',\n",
    "        csv_files=[f for f in os.listdir('../data/raw_data/demand/economic/unemployment_rate') if f.endswith('.csv')],\n",
    "        output_path='../data/clean_data/demand/economic/unemployment_rate/msa_unemployment.csv'\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
